# transformer_solver/trainer.py
import torch
from tqdm import tqdm
import os
import time # ğŸ’¡ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´ time ëª¨ë“ˆ ì¶”ê°€

# ğŸ’¡ 1. í•„ìš”í•œ ëª¨ë“ˆë“¤ì„ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from torch.utils.data import DataLoader
from tensordict import TensorDict


from common.utils.common import TimeEstimator, clip_grad_norms, unbatchify
from .model import PocatModel
from .pocat_env import PocatEnv
from .pocat_dataset import PocatDataset # ğŸ’¡ 2. ìƒˆë¡œ ë§Œë“  Dataset í´ë˜ìŠ¤ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from common.pocat_visualizer import print_and_visualize_one_solution

from common.pocat_classes import Battery, LDO, BuckConverter, Load
from common.pocat_defs import PocatConfig, NODE_TYPE_IC
from common.config_loader import load_configuration_from_file

def tensordict_collate_fn(batch):
    """
    DataLoaderë¡œë¶€í„° ë°›ì€ TensorDict ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë°°ì¹˜ TensorDictë¡œ ìŒ“ìŠµë‹ˆë‹¤.
    """
    # batchëŠ” [TensorDict_sample1, TensorDict_sample2, ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
    return torch.stack(batch, dim=0)

def cal_model_size(model, log_func):
    param_count = sum(param.nelement() for param in model.parameters())
    buffer_count = sum(buffer.nelement() for buffer in model.buffers())
    log_func(f'Total number of parameters: {param_count}')
    log_func(f'Total number of buffer elements: {buffer_count}')

class PocatTrainer:
    # ğŸ’¡ 1. ìƒì„±ìì—ì„œ device ì¸ìë¥¼ ë°›ë„ë¡ ìˆ˜ì •
    def __init__(self, args, env: PocatEnv, device: str):
        self.args = args
        self.env = env
        self.device = device # ì „ë‹¬ë°›ì€ device ì €ì¥
        
        # ğŸ’¡ 2. CUDA ê°•ì œ ì„¤ì • ë¼ì¸ ì‚­ì œ
        # torch.set_default_tensor_type('torch.cuda.FloatTensor') 
        
        # ğŸ’¡ 3. ëª¨ë¸ì„ ìƒì„± í›„, ì§€ì •ëœ deviceë¡œ ì´ë™
        self.model = PocatModel(**args.model_params).to(self.device)
        cal_model_size(self.model, args.log)
        
        # ğŸ’¡ float()ìœ¼ë¡œ ê°ì‹¸ì„œ ê°’ì„ ìˆ«ìë¡œ ê°•ì œ ë³€í™˜í•©ë‹ˆë‹¤.
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=float(args.optimizer_params['optimizer']['lr']),
            weight_decay=float(args.optimizer_params['optimizer'].get('weight_decay', 0)),
        )
        
        if args.optimizer_params['scheduler']['name'] == 'MultiStepLR':
            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(
                self.optimizer,
                milestones=args.optimizer_params['scheduler']['milestones'],
                gamma=args.optimizer_params['scheduler']['gamma']
            )
        else:
            raise NotImplementedError
            
        self.start_epoch = 1

        # ğŸ’¡ ëª¨ë¸ ë¡œë”© ë¡œì§ ì¶”ê°€
        if args.load_path is not None:
            args.log(f"Loading model checkpoint from: {args.load_path}")
            checkpoint = torch.load(args.load_path, map_location=device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            # í›ˆë ¨ì„ ì´ì–´ì„œ í•  ê²½ìš° optimizer ìƒíƒœë„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŒ
            # self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            # self.start_epoch = checkpoint['epoch'] + 1       
            # 
                # ğŸ’¡ 3. í•™ìŠµì„ ìœ„í•œ Datasetê³¼ DataLoaderë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        if not args.test_only:
            train_dataset = PocatDataset(
                generator=self.env.generator,
                steps_per_epoch=args.trainer_params['train_step']
            )
            
            # os.cpu_count()ë¥¼ ì‚¬ìš©í•´ ì‚¬ìš© ê°€ëŠ¥í•œ ì½”ì–´ ìˆ˜ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
            # ì½”ì–´ì˜ ì ˆë°˜ ì •ë„ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•ˆì „í•œ ì‹œì‘ì ì…ë‹ˆë‹¤.
            num_workers = os.cpu_count() // 2 if os.cpu_count() else 4
            args.log(f"ë°ì´í„° ë¡œë”©ì— {num_workers}ê°œì˜ CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

            # ğŸ’¡ 3. collate_fnì— ìƒˆë¡œ ì •ì˜í•œ top-level í•¨ìˆ˜ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.
            self.train_dataloader = DataLoader(
                train_dataset,
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=num_workers,
                pin_memory=True,
                collate_fn=tensordict_collate_fn 
            )

        self.time_estimator = TimeEstimator(log_fn=args.log)

        

    def run(self):
        args = self.args
        self.time_estimator.reset(self.start_epoch)
        
        if args.test_only:
            self.test()
            return

        for epoch in range(self.start_epoch, args.trainer_params['epochs'] + 1):
            args.log('=================================================================')
            
            self.model.train()
            
            # ğŸ’¡ 4. í•™ìŠµ ë£¨í”„ë¥¼ DataLoader ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
            train_pbar = tqdm(self.train_dataloader, 
                              desc=f"Epoch {epoch}/{args.trainer_params['epochs']}", 
                              ncols=140)
            
            total_loss = 0.0
            total_cost = 0.0
            step_count = 0


            for td in train_pbar:
                step_count += 1
                self.optimizer.zero_grad()

                # DataLoaderê°€ ì¤€ë¹„ëœ ë°°ì¹˜ë¥¼ ì œê³µí•˜ë¯€ë¡œ, ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê¸°ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.
                td = td.to(self.device)

                base_desc = f"Epoch {epoch} (Step {step_count})"
                status_message = f"ğŸ”„ Env Reset (ing..)"
                
                # ì§„í–‰ë¥  í‘œì‹œì¤„ ë¡œê¹…ì€ ì´ì œ ëª¨ë¸ì˜ forward íŒ¨ìŠ¤ ë‚´ë¶€ì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
                # ì—¬ê¸°ì„œëŠ” ìƒíƒœ ë©”ì‹œì§€ë¥¼ ë‹¨ìˆœí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                base_desc = f"Epoch {epoch} (Step {step_count})"
                
                # --- ğŸ‘‡ [í•µì‹¬] tqdm ì„¤ëª… ì„¤ì •ê³¼ ë™ì‹œì— ë¡œê·¸ ê¸°ë¡ ---
                train_pbar.set_description(f"{base_desc} | {status_message}")
                args.log(train_pbar.desc)

                reset_start_time = time.time()
                td = self.env.reset(batch_size=args.batch_size)
                reset_time = time.time() - reset_start_time
                
                status_message = f"ğŸ”„ Env Reset (done)"
                train_pbar.set_description(f"{base_desc} | {status_message}")
                args.log(train_pbar.desc)

                model_start_time = time.time()
                # --- ğŸ‘‡ [í•µì‹¬] log í•¨ìˆ˜ë¥¼ ëª¨ë¸ì— ì „ë‹¬ ---
                out = self.model(td, decode_type='sampling', pbar=train_pbar, status_msg=status_message, log_fn=args.log)
                model_time = time.time() - model_start_time

                status_message += f" | â–¶ Encoding (done) | â—€ Decoding (done)"
                status_message += f" | ğŸ“‰ Loss & Bwd (ing..)"
                train_pbar.set_description(f"{base_desc} | {status_message}")
                args.log(train_pbar.desc)
                
                bwd_start_time = time.time()
                num_starts = self.env.generator.num_loads
                reward = out["reward"].view(num_starts, -1)
                log_likelihood = out["log_likelihood"].view(num_starts, -1)
                
                advantage = reward - reward.mean(dim=0, keepdims=True)
                loss = -(advantage * log_likelihood).mean()
                loss.backward()
                bwd_time = time.time() - bwd_start_time

                clip_grad_norms(self.optimizer.param_groups, 1.0)
                self.optimizer.step()
                
                best_reward, _ = reward.max(dim=0)
                current_cost = -best_reward.mean().item()

                total_loss += loss.item()
                total_cost += current_cost
                
                train_pbar.set_postfix({
                    'Loss': f'{total_loss/step_count:.4f}',
                    'Cost': f'${total_cost/step_count:.2f}',
                    'T_Reset': f'{reset_time*1000:.0f}ms',
                    'T_Model': f'{model_time:.2f}s',
                    'T_Bwd': f'{bwd_time*1000:.0f}ms'
                })
            
            final_desc = f"Epoch {epoch}/{args.trainer_params['epochs']} | Done"
            train_pbar.set_description(final_desc)
            args.log(final_desc) # ì—í­ ì¢…ë£Œ ë©”ì‹œì§€ë„ ë¡œê·¸ì— ê¸°ë¡

            self.scheduler.step()
            self.time_estimator.print_est_time(epoch, args.trainer_params['epochs'])
            
            if (epoch % args.trainer_params['model_save_interval'] == 0) or (epoch == args.trainer_params['epochs']):
                save_path = os.path.join(args.result_dir, f'epoch-{epoch}.pth')
                args.log(f"Saving model at epoch {epoch} to {save_path}")
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                }, save_path)

        args.log(" *** Training Done *** ")


    # ... (test, visualize_result ë©”ì†Œë“œëŠ” ê¸°ì¡´ê³¼ ë™ì¼) ...
    @torch.no_grad()
    def test(self):
        args = self.args
        args.log("==================== INFERENCE START ====================")
        self.model.eval()

        td = self.env.reset(batch_size=1) # í…ŒìŠ¤íŠ¸ëŠ” ë°°ì¹˜ 1ë¡œ ê³ ì •
        
        # --- ğŸ‘‡ [í•µì‹¬] í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” 'greedy' ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ í˜¸ì¶œ ---
        out = self.model(td, self.env, decode_type='greedy')

        num_starts = self.env.generator.num_loads
        reward = unbatchify(out["reward"], num_starts)
        actions = unbatchify(out["actions"], num_starts)

        best_reward, best_idx = reward.max(dim=1)
        best_action_sequence = actions[0, best_idx.item()]
        final_cost = -best_reward.item()

        args.log(f"Generated Power Tree Cost: ${final_cost:.4f}")
        
        self.visualize_result(best_action_sequence, final_cost)


    def visualize_result(self, actions, cost):
        """ëª¨ë¸ì´ ìƒì„±í•œ action_sequenceë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
        
        # --- ğŸ’¡ 1. config.jsonì„ ë‹¤ì‹œ ë¡œë“œí•˜ëŠ” ëŒ€ì‹ , generatorì˜ í™•ì¥ëœ configë¥¼ ì‚¬ìš© ---
        config = self.env.generator.config
        battery = Battery(**config.battery)
        constraints = config.constraints
        loads = [Load(**ld) for ld in config.loads]
        
        # Generatorê°€ ë™ì  ë³µì œí•œ ì „ì²´ IC ëª©ë¡(dict)ì„ ê°€ì ¸ì˜´
        expanded_ic_configs = config.available_ics
        
        # ì‹œê°í™”ë¥¼ ìœ„í•´ dictë¥¼ PowerIC ê°ì²´ë¡œ ë³€í™˜
        candidate_ics = []
        for ic_data in expanded_ic_configs:
            ic_type = ic_data.get('type')
            if ic_type == 'LDO':
                candidate_ics.append(LDO(**ic_data))
            elif ic_type == 'Buck':
                candidate_ics.append(BuckConverter(**ic_data))
        # --- ìˆ˜ì • ì™„ë£Œ ---

        node_names = config.node_names
        
        active_edges = []
        used_ic_names = set()
        for action in actions:
            child_idx, parent_idx = action[0].item(), action[1].item()
            child_name = node_names[child_idx]
            parent_name = node_names[parent_idx]
            
            active_edges.append((parent_name, child_name))
            
            if config.node_types[parent_idx] == NODE_TYPE_IC:
                 used_ic_names.add(parent_name)

        solution = {
            "cost": cost,
            "used_ic_names": used_ic_names,
            "active_edges": active_edges
        }
        
        print("\n--- Generated Power Tree (Transformer) ---")
        
        print_and_visualize_one_solution(
            solution=solution, 
            candidate_ics=candidate_ics, # ğŸ’¡ í™•ì¥/ë³€í™˜ëœ IC ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
            loads=loads, 
            battery=battery, 
            constraints=constraints, 
            solution_index=1
        )