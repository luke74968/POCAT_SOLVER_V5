# transformer_solver/pocat_env.py
import torch
from tensordict import TensorDict
from torchrl.envs import EnvBase
from typing import Optional, Tuple
from torchrl.data import UnboundedContinuousTensorSpec as Unbounded, \
    UnboundedDiscreteTensorSpec as UnboundedDiscrete, \
    DiscreteTensorSpec as Categorical, \
    CompositeSpec as Composite

from common.pocat_defs import (
    NODE_TYPE_BATTERY, NODE_TYPE_IC, NODE_TYPE_LOAD,
    FEATURE_DIM, FEATURE_INDEX
)


class PocatEnv(EnvBase):
    name = "pocat"

    def __init__(self, generator_params: dict = {}, device: str = "cpu", **kwargs):
        super().__init__(device=device)
        from .pocat_generator import PocatGenerator
        self.generator = PocatGenerator(**generator_params)
        self._make_spec()
        self._set_seed(None) # ìƒì„±ìžì—ì„œ í˜¸ì¶œì€ ë˜ì–´ ìžˆìœ¼ë‚˜, ì•„ëž˜ì— ë©”ì†Œë“œ ì •ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.

    # --- ðŸ‘‡ 1. ëˆ„ë½ëœ _make_spec ë©”ì†Œë“œ ì¶”ê°€ ---
    def _make_spec(self):
        """í™˜ê²½ì˜ observation, action, reward ìŠ¤íŽ™ì„ ì •ì˜í•©ë‹ˆë‹¤."""
        num_nodes = self.generator.num_nodes
        
        # ê´€ì¸¡ ê³µê°„(Observation Space) ì •ì˜
        self.observation_spec = Composite({
            "nodes": Unbounded(
                shape=(num_nodes, FEATURE_DIM),
                dtype=torch.float32,
            ),
            "prompt_features": Unbounded(
                shape=(5,), # ambient_temp, max_sleep_current, current_margin, thermal_margin_percent, power_sequence_count
                dtype=torch.float32,
            ),
            "adj_matrix": Unbounded(
                shape=(num_nodes, num_nodes),
                dtype=torch.bool,
            ),
            "main_tree_mask": Unbounded(
                shape=(num_nodes,),
                dtype=torch.bool,
            ),
            "ic_current_draw": Unbounded(
                shape=(num_nodes,),
                dtype=torch.float32,
            ),
            "decoding_phase": Categorical(
                shape=(1,),
                n=2, # 0: ìƒˆ Load ì„ íƒ, 1: Trajectory êµ¬ì¶•
                dtype=torch.long,
            ),
            "trajectory_head": UnboundedDiscrete(
                shape=(1,),
                dtype=torch.long,
            ),
            "unconnected_loads_mask": Unbounded(
                shape=(num_nodes,),
                dtype=torch.bool,
            ),
            "step_count": UnboundedDiscrete(
                shape=(1,),
                dtype=torch.long,
            ),
        })
        
        # í–‰ë™ ê³µê°„(Action Space) ì •ì˜: [ìžì‹ ë…¸ë“œ, ë¶€ëª¨ ë…¸ë“œ]
        self.action_spec = UnboundedDiscrete(
            shape=(2,),
            dtype=torch.long,
        )
        
        # ë³´ìƒ(Reward) ìŠ¤íŽ™ ì •ì˜
        self.reward_spec = Unbounded(shape=(1,))

    # --- ðŸ‘‡ 2. ëˆ„ë½ëœ _set_seed ë©”ì†Œë“œ ì¶”ê°€ ---
    def _set_seed(self, seed: Optional[int] = None):
        """í™˜ê²½ì˜ ëžœë¤ ì‹œë“œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (torchrl í•„ìˆ˜ êµ¬í˜„)"""
        # í˜„ìž¬ í™˜ê²½ì€ ìžì²´ì ì¸ ëžœë¤ ìš”ì†Œê°€ ì—†ìœ¼ë¯€ë¡œ íŠ¹ë³„í•œ ë¡œì§ì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
        # í•˜ì§€ë§Œ EnvBaseë¥¼ ìƒì†ë°›ê¸° ìœ„í•´ ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
        if seed is not None:
            torch.manual_seed(seed)

    # --- ðŸ‘‡ 1. ëˆ„ë½ë˜ì—ˆë˜ select_start_nodes ë©”ì†Œë“œ ì¶”ê°€ ---
    def select_start_nodes(self, td: TensorDict) -> Tuple[int, torch.Tensor]:
        """POMO decodingì„ ìœ„í•´ ì‹œìž‘ ë…¸ë“œ(ëª¨ë“  Load)ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
        # ë…¸ë“œ íƒ€ìž… ì •ë³´ëŠ” ë°°ì¹˜ ë‚´ì—ì„œ ë™ì¼í•˜ë¯€ë¡œ 0ë²ˆ ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        node_types = td["nodes"][0, :, :FEATURE_INDEX["node_type"][1]].argmax(-1)
        start_nodes_idx = torch.where(node_types == NODE_TYPE_LOAD)[0]
        num_starts = len(start_nodes_idx)
        return num_starts, start_nodes_idx

    # --- ðŸ‘‡ 2. ëˆ„ë½ë˜ì—ˆë˜ ê²½ë¡œ ì¶”ì  í—¬í¼ ë©”ì†Œë“œë“¤ ì¶”ê°€ ---
    def _trace_path(self, b_idx: int, start_node: int, adj_matrix: torch.Tensor) -> list[int]:
        """ë‹¨ì¼ ë°°ì¹˜ í•­ëª©ì— ëŒ€í•´ start_nodeì—ì„œ ì‹œìž‘í•˜ëŠ” ê²½ë¡œë¥¼ ì—­ì¶”ì í•˜ì—¬ ë…¸ë“œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        path = [start_node]
        current_node = start_node
        # adj_matrix[b_idx, parent, child] í˜•íƒœì´ë¯€ë¡œ, current_nodeë¥¼ ìžì‹ìœ¼ë¡œ ê°–ëŠ” ë¶€ëª¨ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        while True:
            parents = adj_matrix[b_idx, :, current_node].nonzero(as_tuple=True)[0]
            if parents.numel() == 0:
                break
            parent_node = parents[0].item() # ê²½ë¡œëŠ” í•˜ë‚˜ë¿ì´ë¼ê³  ê°€ì •
            path.append(parent_node)
            current_node = parent_node
        return path

    def _trace_path_batch(self, b_idx: torch.Tensor, start_nodes: torch.Tensor, adj_matrix: torch.Tensor) -> torch.Tensor:
        """ë°°ì¹˜ ì „ì²´ì— ëŒ€í•´ start_nodeë“¤ì˜ ëª¨ë“  ì¡°ìƒì„ ì°¾ì•„ ë§ˆìŠ¤í¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        num_nodes = adj_matrix.shape[-1]
        
        # ì„ íƒëœ ë°°ì¹˜ í•­ëª©ë“¤ì— ëŒ€í•œ ì¸ì ‘ í–‰ë ¬
        adj_b = adj_matrix[b_idx]
        
        # ê²½ë¡œ ë§ˆìŠ¤í¬ ì´ˆê¸°í™” (ì‹œìž‘ ë…¸ë“œë§Œ True)
        path_mask = torch.zeros(len(b_idx), num_nodes, dtype=torch.bool, device=self.device)
        path_mask[torch.arange(len(b_idx)), start_nodes] = True
        
        # í–‰ë ¬ ê³±ì…ˆì„ ì´ìš©í•´ ê·¸ëž˜í”„ë¥¼ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ëª¨ë“  ì¡°ìƒì„ ì°¾ìŠµë‹ˆë‹¤.
        for _ in range(num_nodes):
            # í˜„ìž¬ ê²½ë¡œì— í¬í•¨ëœ ë…¸ë“œë“¤ì˜ ë¶€ëª¨ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            parents_mask = (adj_b.float() @ path_mask.float().unsqueeze(-1)).squeeze(-1) > 0
            
            # ë” ì´ìƒ ìƒˆë¡œìš´ ë¶€ëª¨ê°€ ì—†ìœ¼ë©´ (ê²½ë¡œì˜ ëì— ë„ë‹¬í•˜ë©´) ì¢…ë£Œí•©ë‹ˆë‹¤.
            if (parents_mask & ~path_mask).sum() == 0:
                break
            
            # ìƒˆë¡œ ì°¾ì€ ë¶€ëª¨ë“¤ì„ ê²½ë¡œ ë§ˆìŠ¤í¬ì— ì¶”ê°€í•©ë‹ˆë‹¤.
            path_mask |= parents_mask
            
        return path_mask            

    def _reset(self, td: Optional[TensorDict] = None, **kwargs) -> TensorDict:
        if td is None:
            batch_size = kwargs.get("batch_size", self.batch_size)
            if not isinstance(batch_size, int): batch_size = batch_size[0]
            td = self.generator(batch_size=batch_size).to(self.device)
            
        num_nodes = td["nodes"].shape[1]
        batch_size = td.batch_size[0]
        
        # --- ðŸ’¡ 1. Trajectory ê¸°ë°˜ ìƒíƒœ(state) ìž¬ì •ì˜ ---
        reset_td = TensorDict({
            "nodes": td["nodes"],
            "prompt_features": td["prompt_features"],
            "adj_matrix": torch.zeros(batch_size, num_nodes, num_nodes, dtype=torch.bool, device=self.device),
            "main_tree_mask": torch.zeros(batch_size, num_nodes, dtype=torch.bool, device=self.device),
            "ic_current_draw": torch.zeros(batch_size, num_nodes, device=self.device),
            
            # --- ìƒˆë¡œìš´ ìƒíƒœ ë³€ìˆ˜ ---
            # 0: ìƒˆ Load ì„ íƒ ë‹¨ê³„, 1: Trajectory(ê²½ë¡œ) êµ¬ì¶• ë‹¨ê³„
            "decoding_phase": torch.zeros(batch_size, 1, dtype=torch.long, device=self.device),
            # í˜„ìž¬ ë§Œë“¤ê³  ìžˆëŠ” ê²½ë¡œì˜ ê°€ìž¥ ë ë…¸ë“œ (Loadì—ì„œ ë°°í„°ë¦¬ ë°©í–¥ìœ¼ë¡œ)
            "trajectory_head": torch.full((batch_size, 1), -1, dtype=torch.long, device=self.device),
            # ì•„ì§ íŠ¸ë¦¬ì— ì—°ê²°ë˜ì§€ ì•Šì€ Loadë“¤ì˜ ë§ˆìŠ¤í¬
            "unconnected_loads_mask": torch.ones(batch_size, num_nodes, dtype=torch.bool, device=self.device),
            
            "step_count": torch.zeros(batch_size, 1, dtype=torch.long, device=self.device),
        }, batch_size=[batch_size], device=self.device)
        
        reset_td.set("done", torch.zeros(batch_size, 1, dtype=torch.bool, device=self.device))
        
        # ë°°í„°ë¦¬(ì¸ë±ìŠ¤ 0)ëŠ” í•­ìƒ ë©”ì¸ íŠ¸ë¦¬ì— í¬í•¨
        reset_td["main_tree_mask"][:, 0] = True
        
        node_types = td["nodes"][0, :, :FEATURE_INDEX["node_type"][1]].argmax(-1)
        is_load = node_types == NODE_TYPE_LOAD
        reset_td["unconnected_loads_mask"][:, ~is_load] = False
        
        return reset_td

    # ðŸ’¡ ì¶”ê°€ëœ step ë©”ì†Œë“œ: ë°°ì¹˜ í¬ê¸° ê²€ì‚¬ë¥¼ ìš°íšŒí•©ë‹ˆë‹¤.
    def step(self, tensordict: TensorDict) -> TensorDict:
        return self._step(tensordict)

    def _step(self, td: TensorDict) -> TensorDict:
        action = td["action"]
        b_idx = torch.arange(td.batch_size[0], device=self.device)
        phase = td["decoding_phase"].squeeze(-1)

        next_obs = td.clone()

        phase0_mask = phase == 0
        phase1_mask = phase == 1
        b_phase0 = b_idx[phase0_mask]
        b_phase1 = b_idx[phase1_mask]

        if b_phase0.numel() > 0:  # ìƒˆ Load ì„ íƒ ë‹¨ê³„
            selected_load = action[b_phase0, 0]
            next_obs["trajectory_head"][b_phase0] = selected_load.unsqueeze(-1)
            next_obs["unconnected_loads_mask"][b_phase0, selected_load] = False
            next_obs["decoding_phase"][b_phase0, 0] = 1  # ë‹¤ìŒì€ ê²½ë¡œ êµ¬ì¶• ë‹¨ê³„ë¡œ

        if b_phase1.numel() > 0:  # Trajectory êµ¬ì¶• ë‹¨ê³„
            child_idx, parent_idx = action[b_phase1, 0], action[b_phase1, 1]
            next_obs["adj_matrix"][b_phase1[:, None], parent_idx, child_idx] = True

            # [ìˆ˜ì •] ì „ë¥˜ ì „íŒŒ ë¡œì§ êµ¬í˜„
            path_nodes_mask = self._trace_path_batch(b_phase1, child_idx, next_obs["adj_matrix"])
            path_nodes_currents = (
                td["nodes"][b_phase1] * path_nodes_mask.unsqueeze(-1)
            )[:, :, FEATURE_INDEX["current_active"]]

            for idx, b in enumerate(b_phase1.tolist()):
                total_child_current = path_nodes_currents[idx].sum()
                ancestor = parent_idx[idx].item()
                while ancestor != 0:
                    next_obs["ic_current_draw"][b, ancestor] += total_child_current
                    ancestors_of_ancestor = next_obs["adj_matrix"][b, :, ancestor].nonzero(as_tuple=True)[0]
                    if ancestors_of_ancestor.numel() == 0:
                        break
                    ancestor = ancestors_of_ancestor[0].item()

            is_parent_in_main_tree = next_obs["main_tree_mask"][b_phase1, parent_idx]

            for idx, b in enumerate(b_phase1.tolist()):
                if is_parent_in_main_tree[idx]:
                    path_nodes_indices = self._trace_path(b, child_idx[idx], next_obs["adj_matrix"])
                    next_obs["main_tree_mask"][b, path_nodes_indices] = True
                    if next_obs["unconnected_loads_mask"][b].sum() == 0:
                        next_obs["done"][b] = True
                    else:
                        next_obs["decoding_phase"][b] = 0
                else:
                    next_obs["trajectory_head"][b] = parent_idx[idx]

        next_obs.set("step_count", td["step_count"] + 1)
        
        return TensorDict({
            "next": next_obs,
            "reward": self.get_reward(next_obs),
            "done": next_obs["done"],
        }, batch_size=td.batch_size)
    
    def get_action_mask(self, td: TensorDict) -> torch.Tensor:
        batch_size, num_nodes, _ = td["nodes"].shape
        phase = td["decoding_phase"].squeeze(-1)

        mask = torch.zeros(batch_size, num_nodes, num_nodes, dtype=torch.bool, device=self.device)

        phase0_idx = torch.where(phase == 0)[0]
        phase1_idx = torch.where(phase == 1)[0]

        if phase0_idx.numel() > 0:
            # ìžì‹: ì•„ì§ ì—°ê²° ì•ˆëœ Loadë§Œ ê°€ëŠ¥, ë¶€ëª¨: ì‚¬ìš© ì•ˆí•¨ (0ìœ¼ë¡œ ê³ ì •)
            mask[phase0_idx, :, 0] = td["unconnected_loads_mask"][phase0_idx]

        if phase1_idx.numel() > 0:
            b_idx = phase1_idx
            child_indices = td["trajectory_head"][b_idx].squeeze(-1)

            # [ìˆ˜ì •] 'can_be_parent' ë³€ìˆ˜ ì´ˆê¸°í™”
            can_be_parent = torch.ones(len(b_idx), num_nodes, dtype=torch.bool, device=self.device)

            node_types = td["nodes"][0, :, :FEATURE_INDEX["node_type"][1]].argmax(-1)
            is_ic = node_types == NODE_TYPE_IC

            current_path_mask = self._trace_path_batch(b_idx, child_indices, td["adj_matrix"])

            # ê¸°ë³¸ í›„ë³´: (ì•„ì§ ì‚¬ìš©ë˜ì§€ ì•Šì€ IC) ë˜ëŠ” (ì´ë¯¸ ë©”ì¸ íŠ¸ë¦¬ì— ì†í•œ ë…¸ë“œ)
            can_be_parent &= (
                is_ic.unsqueeze(0) & ~current_path_mask & ~td["main_tree_mask"][b_idx]
            ) | td["main_tree_mask"][b_idx]

            # ìžê¸° ìžì‹ , ì´ë¯¸ ê²½ë¡œì— í¬í•¨ëœ ë…¸ë“œëŠ” ë¶€ëª¨ê°€ ë  ìˆ˜ ì—†ìŒ
            can_be_parent[torch.arange(len(b_idx), device=self.device), child_indices] = False

            # 1. ì „ì•• í˜¸í™˜ì„±
            head_node_vin_min = td["nodes"][b_idx, child_indices, FEATURE_INDEX["vin_min"]]
            parent_vout_max = td["nodes"][b_idx, :, FEATURE_INDEX["vout_max"]]
            can_be_parent &= parent_vout_max >= head_node_vin_min.unsqueeze(1)

            # 2. ì „ë¥˜ í•œê³„
            path_nodes_currents = (
                td["nodes"][b_idx, :, FEATURE_INDEX["current_active"]] * current_path_mask
            ).sum(dim=1)
            prospective_draw = td["ic_current_draw"][b_idx] + path_nodes_currents.unsqueeze(1)
            parent_limits = td["nodes"][b_idx, :, FEATURE_INDEX["i_limit"]]
            inf_limits = torch.where(parent_limits > 0, parent_limits, float("inf"))
            can_be_parent &= prospective_draw <= inf_limits

            # 3. ì „ì—­ ì œì•½ì¡°ê±´ (Independent Rail, Power Sequence)
            constraints = self.generator.config.constraints
            loads_info = self.generator.config.loads
            node_names = self.generator.config.node_names

            # ì¡°ìƒ(ancestor) í–‰ë ¬ ê³„ì‚°: í˜„ìž¬ ë©”ì¸ íŠ¸ë¦¬ì— ëŒ€í•´ì„œë§Œ ê³„ì‚°
            ancestors = td["adj_matrix"][b_idx].clone()
            for k in range(num_nodes):
                for i in range(num_nodes):
                    for j in range(num_nodes):
                        ancestors[:, i, j] |= ancestors[:, i, k] & ancestors[:, k, j]

            # 3a. Independent Rail
            head_load_idx = child_indices - (1 + len(self.generator.config.available_ics))
            for idx, b in enumerate(b_idx.tolist()):
                if 0 <= head_load_idx[idx] < len(loads_info):
                    load = loads_info[head_load_idx[idx]]
                    rail_type = load.get("independent_rail_type")
                    if rail_type == "exclusive_supplier":
                        num_children = td["adj_matrix"][b].sum(dim=1)
                        can_be_parent[idx] &= num_children == 0
                    elif rail_type == "exclusive_path":
                        # ê²½ë¡œì— ì†í•  ëª¨ë“  ìž ìž¬ì  ë¶€ëª¨ë“¤ì´ ë‹¤ë¥¸ ìžì‹ì„ ê°€ì§€ë©´ ì•ˆë¨
                        num_children = td["adj_matrix"][b].sum(dim=1)
                        can_be_parent[idx] &= num_children <= 1

            # 3b. Power Sequence
            for seq in constraints.get("power_sequences", []):
                if seq.get("f") != 1:
                    continue
                j_name, k_name = seq["j"], seq["k"]
                if j_name not in node_names or k_name not in node_names:
                    continue
                j_idx, k_idx = node_names.index(j_name), node_names.index(k_name)

                # í˜„ìž¬ headê°€ k_idxì¸ ê²½ìš°, jì˜ ì¡°ìƒì´ ë  ìˆ˜ ìžˆëŠ” ë…¸ë“œëŠ” ë¶€ëª¨ê°€ ë  ìˆ˜ ì—†ìŒ
                is_head_k = child_indices == k_idx
                if is_head_k.any():
                    j_ancestors = ancestors[is_head_k, :, j_idx]
                    can_be_parent[is_head_k] &= ~j_ancestors

            mask[b_idx, :, child_indices] = can_be_parent

        return mask
    
    def get_reward(self, td: TensorDict) -> torch.Tensor:
        """
        Calculates the reward based on the final state of the power tree.
        The reward is the negative of the total cost of used ICs.
        This function is called only when an episode is done.
        """
        reward = torch.zeros(td.batch_size[0], device=self.device)
        done = td["done"].squeeze(-1)
        
        if done.any():
            # Calculate cost based on the final adjacency matrix
            is_used_mask = td["adj_matrix"][done].any(dim=1) | td["adj_matrix"][done].any(dim=2)
            
            node_costs = td["nodes"][done, :, FEATURE_INDEX["cost"]]
            ic_mask = td["nodes"][done, :, FEATURE_INDEX["node_type"][0] + NODE_TYPE_IC] == 1
            
            used_ic_mask = is_used_mask & ic_mask
            total_cost = (node_costs * used_ic_mask).sum(dim=-1)
            reward[done] = -total_cost
            
            # (Optional) Add penalty for violating sleep current constraint
            max_sleep_current = self.generator.config.constraints.get("max_sleep_current", 0.0)
            if max_sleep_current > 0:
                loads_info = self.generator.config.loads
